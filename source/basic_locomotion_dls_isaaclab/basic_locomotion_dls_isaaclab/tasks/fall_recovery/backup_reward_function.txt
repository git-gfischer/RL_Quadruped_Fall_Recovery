  def _get_rewards(self) -> torch.Tensor:
        """
        Rewards exactly as in the paper's table:

        Orientation/Posture:
        - Base Orientation:      g_xy^2
        - Upright Orientation:   exp(- (g_z+1)^2 / (2 eps^2))
        - Target Posture:        exp(-||q - q_stand||^2)   only if |g_z+1| < eps

        Contact Management:
        - Feet Contact:          sum_i I_contact_i  (i=1..4)
        - Body Contact:          I_contact (base/hip/thigh)

        Stability Control:
        - Safety Force:          sum_i ||f_i^{xy}||_2      (feet)
        - Body-bias:             clip( ||p_xy - p_xy_init||_2 , 0, 4 )

        Motion Constraints:
        - Position Limits:       sum_i 1[q_i > q_max or q_i < q_min]
        - Angular Vel Limit:     sum_i max(|qd_i| - 0.8, 0)
        - Joint Acceleration:    ||qdd||_2^2
        - Joint Velocity:        ||qd||_2^2
        - Action Smoothing:      ||a_t - a_{t-1}||_2^2
        - Joint Torques:         ||tau||_2^2
        """
        dt = self.step_dt

        # ---------- lazy state needed by body-bias ----------
        if not hasattr(self, "_init_root_xy"):
            self._init_root_xy = self._robot.data.root_state_w[:, :2].clone()

        # ---------- signals ----------
        g_b  = self._robot.data.projected_gravity_b             # (N,3)
        q    = self._robot.data.joint_pos                       # (N,J)
        qd   = self._robot.data.joint_vel                       # (N,J)
        qdd  = self._robot.data.joint_acc                       # (N,J)
        tau  = self._robot.data.applied_torque                  # (N,J)
        a_t   = self._actions
        a_tm1 = self._previous_actions

        # current contact forces
        F_w   = self._contact_sensor.data.net_forces_w          # (N, bodies, 3)
        Fmag  = torch.linalg.norm(F_w, dim=-1)                  # (N, bodies)

        # feet contact (binary) and horizontal forces for safety-force
        feet_ids = self._feet_ids
        foot_F   = F_w[:, feet_ids, :2]                         # (N,4,2)
        safety_force = torch.norm(foot_F, dim=-1).sum(dim=1)    # Î£ ||f_i^xy||, (N,)
        feet_contact = (Fmag[:, feet_ids].max(dim=1).values > 1.0).float()  # (N,)

        # body contact (base + hips + thighs)
        undesired_ids = self._undesired_contact_body_ids
        body_contact = (Fmag[:, undesired_ids].max(dim=1).values > 1.0).float()  # (N,)

        # body-bias distance in XY (clipped to 4)
        p_xy   = self._robot.data.root_state_w[:, :2]
        disp   = torch.norm(p_xy - self._init_root_xy, dim=1)                   # (N,)
        body_bias = torch.clamp(disp, 0.0, 4.0)

        # position limits (count of violations)
        pos_limit_term = torch.zeros(self.num_envs, device=self.device)
        limits = getattr(self._robot.data, "joint_pos_limits", None)
        if limits is not None:
            limits = torch.as_tensor(limits, device=self.device, dtype=q.dtype)
            if limits.dim() == 2:  # (J,2) -> (N,J,2)
                limits = limits.unsqueeze(0).expand(self.num_envs, -1, -1)
            lower, upper = limits[..., 0], limits[..., 1]
            pos_limit_term = ((q < lower) | (q > upper)).float().sum(dim=1)

        # angular velocity limit excess over 0.8 rad/s
        qd_excess = torch.clamp(qd.abs() - 0.8, min=0.0).sum(dim=1)

        # quadratic costs
        qdd_sq = (qdd ** 2).sum(dim=1) # joint acceleration squared
        qd_sq  = (qd  ** 2).sum(dim=1) # joint velocity squared
        act_smooth = ((a_t - a_tm1) ** 2).sum(dim=1) # action smoothing squared
        tau_sq = (tau ** 2).sum(dim=1) # joint torque squared

        # ---------- orientation/posture ----------
        gxy_sq = (g_b[:, :2] ** 2).sum(dim=1)                  # ||g_xy||^2
        gz = g_b[:, 2]
        eps_upright = float(getattr(self.cfg, "upright_eps", 0.10))
        upright = torch.exp(-((gz + 1.0) ** 2) / (2.0 * eps_upright * eps_upright))
        near_upright = (gz + 1.0).abs() < eps_upright
        # Stance phase definition - simple upright hold
        if not hasattr(self, "_upright_hold_counter") or self._upright_hold_counter.shape[0] != self.num_envs:
            self._upright_hold_counter = torch.zeros(self.num_envs, dtype=torch.long, device=self.device)
        
        # Stance phase: robot holds upright for a short period
        hold_upright_steps = max(1, int(0.3 / float(self.step_dt)))  # ~0.3s hold requirement
        self._upright_hold_counter = torch.where(near_upright, self._upright_hold_counter + 1, torch.zeros_like(self._upright_hold_counter))
        stance_phase = self._upright_hold_counter >= hold_upright_steps

        q_stand = torch.tensor(self.cfg.stand_joint_pos, device=self.device)
        posture_err_sq = ((q - q_stand) ** 2).sum(dim=1)
        
        # Posture attraction only during stance phase - moderate pull to stance
        target_posture = torch.exp(-posture_err_sq) * stance_phase.float()   
        

        # ---------- height reward (only when upright) ----------
        current_height = self._robot.data.root_state_w[:, 2]  # Z position
        target_height = getattr(self.cfg, "target_height", 0.35)  # Default standing height
        height_error = torch.abs(current_height - target_height)
        height_reward = torch.exp(-height_error / 0.1)  # Exponential reward centered at target height
        
        # Only give height reward during stance phase
        height_reward = height_reward * stance_phase.float()
        
        # Penalty for being upside down (g_z > 0.5)
        upside_down_penalty = torch.clamp(g_b[:, 2] - 0.5, min=0.0)  # Penalty when g_z > 0.5
        
        # Recovery progress reward - encourages getting closer to upright
        recovery_progress = torch.clamp(-g_b[:, 2], min=0.0, max=1.0)  # 0 when upside down, 1 when upright
        
        # Strong orientation reward - heavily penalize any deviation from upright
        orientation_penalty = torch.abs(g_b[:, 2] + 1.0)  # 0 when perfectly upright (g_z = -1), increases with deviation

        # ---------- stance-related signals ----------
        # safe lazy init for stance flag
        if not hasattr(self, "_ever_reached_target") or self._ever_reached_target.shape[0] != self.num_envs:
            self._ever_reached_target = torch.zeros(self.num_envs, dtype=torch.bool, device=self.device)

        # penalize belly/base contact only when near upright
        # belly penalty only during stance phase
        belly_contact_upright = body_contact * stance_phase.float()

        # stance conditions: upright, near target joints, sufficient height, feet support, no body contact
        posture_close = posture_err_sq < 0.5  # More forgiving for "almost perfect"
        height_ok = current_height > (target_height - 0.03)
        feet_support = feet_contact > 0.0
        no_belly = body_contact < 0.5
        stance_ok = posture_close & height_ok & feet_support & no_belly & stance_phase
        # sustained stance counter to avoid one-frame spikes
        if not hasattr(self, "_stance_counter") or self._stance_counter.shape[0] != self.num_envs:
            self._stance_counter = torch.zeros(self.num_envs, dtype=torch.long, device=self.device)
        hold_steps = max(1, int(0.4 / float(self.step_dt)))  # ~0.4s hold
        self._stance_counter = torch.where(stance_ok, self._stance_counter + 1, torch.zeros_like(self._stance_counter))
        stance_held = self._stance_counter >= hold_steps
        newly_stanced = stance_held & (~self._ever_reached_target)
        # update latch when held long enough
        self._ever_reached_target |= stance_held
        
        # Refinement phase: after robot has been stably standing for a while
        if not hasattr(self, "_refinement_counter") or self._refinement_counter.shape[0] != self.num_envs:
            self._refinement_counter = torch.zeros(self.num_envs, dtype=torch.long, device=self.device)
        refinement_delay = max(1, int(2.0 / float(self.step_dt)))  # 2 seconds after stance phase starts
        self._refinement_counter = torch.where(stance_phase, self._refinement_counter + 1, torch.zeros_like(self._refinement_counter))
        refinement_phase = self._refinement_counter >= refinement_delay
        
        # Gentle refinement reward - only when in refinement phase
        refinement_posture = torch.exp(-posture_err_sq * 0.5) * refinement_phase.float()  # Gentle exponential

        # penalize wide leg spread (hip abduction) when near upright
        hip_abduction = q[:, 0:4].abs().mean(dim=1)
        hip_spread_penalty = hip_abduction * stance_phase.float()
        
        # Penalty for feet off the ground - encourage all feet down
        feet_heights = self._robot.data.body_pos_w[:, self._feet_ids_robot, 2]  # Z positions of all feet
        feet_height_mean = feet_heights.mean(dim=1)  # Average foot height
        feet_off_ground_penalty = torch.clamp(feet_height_mean, min=0.0) * stance_phase.float()  # Penalize when feet are above ground

        # ---------- weights (focused on orientation) ----------
        w = dict(
            base_orient=-0.5,
            upright=15.0,  # Much stronger - this is the main goal
            target_posture=3.0,  # Stronger pull into stance when upright
            height_reward=1.0,  # Reduced - only when upright anyway
            upside_down_penalty=-10.0,  # Very strong penalty for upside down
            recovery_progress=5.0,  # Strong reward for any progress toward upright
            orientation_penalty=-8.0,  # Strong penalty for any deviation from upright
            contact_belly_upright=-3.0,  # penalize belly contact when upright
            stance_bonus=25.0,  # bigger one-shot bonus on sustained stance
            hip_spread_penalty=-10.0,  # Strongly discourage wide leg spread when upright (increased)
            feet_off_ground_penalty=-8.0,  # Strongly discourage feet off ground in stance
            refinement_posture=2.0,  # gentle refinement reward for perfect stance
            feet_contact=0.1,  # Minimal - not important during recovery
            body_contact=-0.05,  # Minimal penalty - some contact needed for flipping
            safety_force=-1.0e-3,  # Minimal - allow aggressive movements
            body_bias=-0.02,  # Minimal - robot needs to move around
            pos_limits=-0.2,  # Minimal - allow joint movement
            ang_vel_limit=-0.02,  # Minimal - allow fast movement
            qdd_sq=-5.0e-7,  # Minimal - allow aggressive accelerations
            qd_sq=-2.0e-3,  # Minimal - allow fast joint movement
            act_smooth=-0.002,  # Minimal - allow dynamic actions
            tau_sq=-1.0e-4,  # Minimal - allow high torques
        )

        # ---------- assemble rewards ----------
        rewards = {
            # Orientation / Posture (FOCUSED ON UPRIGHT)
            "r_orient_base_gxy":       w["base_orient"]   * gxy_sq * dt,
            "r_orient_upright":        w["upright"]       * upright * dt,
            "r_orient_target_posture": w["target_posture"]* target_posture * dt,
            "r_height":                w["height_reward"] * height_reward * dt,
            "r_upside_down_penalty":   w["upside_down_penalty"] * upside_down_penalty * dt,
            "r_orientation_penalty":   w["orientation_penalty"] * orientation_penalty * dt,
            "r_contact_belly_upright": w["contact_belly_upright"] * belly_contact_upright * dt,
            "r_stance_bonus":          w["stance_bonus"] * newly_stanced.float(),
            "r_hip_spread_penalty":    w["hip_spread_penalty"] * hip_spread_penalty * dt,
            "r_feet_off_ground_penalty": w["feet_off_ground_penalty"] * feet_off_ground_penalty * dt,
            "r_refinement_posture":    w["refinement_posture"] * refinement_posture * dt,
            
            # Recovery Progress
            "r_recovery_progress":     w["recovery_progress"] * recovery_progress * dt,

            # Contact Management
            "r_contact_feet":          w["feet_contact"]  * feet_contact * dt,
            "r_contact_body":          w["body_contact"]  * body_contact * dt,

            # Stability Control
            "r_stability_safety_force":w["safety_force"]  * safety_force * dt,
            "r_stability_body_bias":   w["body_bias"]     * body_bias * dt,

            # Motion Constraints
            "r_motion_pos_limits":     w["pos_limits"]    * pos_limit_term * dt,
            "r_motion_ang_vel_limit":  w["ang_vel_limit"] * qd_excess * dt,
            "r_motion_joint_acc":      w["qdd_sq"]        * qdd_sq * dt,
            "r_motion_joint_vel":      w["qd_sq"]         * qd_sq * dt,
            "r_motion_action_smooth":  w["act_smooth"]    * act_smooth * dt,
            "r_motion_torque":         w["tau_sq"]        * tau_sq * dt,
        }

        # ---------- sum ----------
        reward = torch.sum(torch.stack(list(rewards.values())), dim=0)

        # ---------- logging (safe) ----------
        if not hasattr(self, "_episode_sums"):
            self._episode_sums = {}
        for k, v in rewards.items():
            # filter by allowlist to reduce logging noise
            if (self._log_metric_keys is None) or (k in self._log_metric_keys):
                buf = self._episode_sums.get(k)
                if buf is None or buf.shape[0] != self.num_envs or buf.device != self.device:
                    self._episode_sums[k] = torch.zeros(self.num_envs, dtype=torch.float, device=self.device)
                self._episode_sums[k] += v
        # accumulate total return
        if not hasattr(self, "_episode_return") or self._episode_return.shape[0] != self.num_envs:
            self._episode_return = torch.zeros(self.num_envs, dtype=torch.float, device=self.device)
        self._episode_return += reward
        #------------------------------------------------

        return reward
